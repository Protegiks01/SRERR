### Title
Nonce Reuse in DKG Confirmation Signing Enables Private Key Share Recovery

### Summary
The DKG confirmation signing protocol reuses cached preprocesses when `DkgConfirmer::complete()` is called multiple times for the same attempt, violating FROST's critical security requirement that nonces must never be reused. If a coordinator crashes between processing DkgConfirmed transactions and committing the block as handled, the same cached preprocess is retrieved and used again on restart, enabling third-party recovery of validators' private key shares.

### Finding Description

The vulnerability exists in the coordinator's DKG confirmation signing implementation. The root cause is that cached preprocesses persist in the database and are retrieved every time signing operations are performed, without any mechanism to prevent reuse.

**Exact Location:** [1](#0-0) 

In `preprocess_internal()`, a cached preprocess is created once and stored in the database. Every subsequent call retrieves this same cached preprocess from the database and reconstructs the signing machine with the same nonces. [2](#0-1) 

The `complete()` method calls `share_internal()` which calls `preprocess_internal()`, retrieving the cached preprocess and calling `machine.sign()`. This happens every time `complete()` is called for the same attempt. [3](#0-2) 

When DkgConfirmed transactions are accumulated, if `complete()` returns an error due to an invalid share from another participant, a RemoveParticipantDueToDkg transaction is published and the function returns early. Critically, `DkgLocallyCompleted` is not set, and the cached preprocess remains in the database. [4](#0-3) 

The block scanner processes blocks sequentially and only updates `LastHandledBlock` after the transaction is committed. If the coordinator crashes after calling `handle_application_tx()` but before `txn.commit()`, the block will be reprocessed on restart, causing `complete()` to be called again with the same cached preprocess.

**Why Existing Mitigations Fail:**

The code includes comments explaining that nonce reuse safety depends on BFT consensus ordering: [5](#0-4) 

However, this safety argument assumes nonces cannot be reused without a complete system rebuild. The cached preprocess mechanism violates this assumption by persisting nonces across multiple invocations of the signing protocol for the same attempt.

**Security Requirement Violation:** [6](#0-5) [7](#0-6) 

### Impact Explanation

**Specific Harm:**
When nonces are reused in FROST signing, an observer who sees both signing attempts can recover the signer's private key share using standard algebraic attacks on Schnorr signatures. This affects all validators participating in DKG confirmation.

**Quantified Impact:**
- Complete compromise of affected validators' private key shares
- If enough key shares are recovered (t of n), the entire multisig controlling external assets is compromised
- Potential loss of all assets (Bitcoin, Ethereum, Monero, etc.) controlled by the validator set
- No user funds are safe if threshold is breached

**Severity Justification:**
This is CRITICAL severity under the audit scope: "Unintended, undocumented recovery of private spend keys (or key shares)". The vulnerability directly enables recovery of private key shares through nonce reuse, which is explicitly documented as enabling "third-party recovery of your private key share."

### Likelihood Explanation

**Required Attacker Capabilities:**
1. Ability to cause at least one validator to provide an invalid DkgConfirmed share (Byzantine validator or network manipulation)
2. Ability to cause or wait for a coordinator crash at the specific timing window
3. Ability to observe both signing attempts (passive network observation)

**Attack Complexity:**
Medium. While the timing window is narrow (between transaction processing and commit), coordinator crashes can occur naturally due to:
- Out of memory conditions
- Power failures
- Software bugs
- Operating system issues

A Byzantine validator can deliberately provide invalid shares to trigger the error path, significantly increasing exploit probability.

**Economic Feasibility:**
Highly profitable. The cost of causing a crash or providing invalid shares is negligible compared to the value of assets controlled by the multisig. Observing network traffic is passive and free.

**Detection Risk:**
Low. The nonce reuse occurs internally within the coordinator's signing protocol. There is no on-chain evidence, and the attack can be executed passively by simply observing failed and retried signing attempts.

### Recommendation

**Immediate Fix:**
Delete the cached preprocess immediately after its first use in signing operations. Modify `share_internal()` to remove the cached preprocess from the database after calling `machine.sign()`:

1. Add a method to delete cached preprocesses:
```rust
impl<T: DbTxn, C: Encode> SigningProtocol<'_, T, C> {
  fn delete_cached_preprocess(&mut self) {
    CachedPreprocesses::del(self.txn, &self.context);
  }
}
```

2. Call it immediately after using the preprocess in `share_internal()`: [8](#0-7) 

After line 178 (after the machine.sign() call completes), add:
```rust
self.signing_protocol().delete_cached_preprocess();
```

**Alternative Mitigation:**
Check `DkgLocallyCompleted` before processing DkgConfirmed transactions: [3](#0-2) 

Add a check at the beginning of DkgConfirmed handling (after line 512):
```rust
if DkgLocallyCompleted::get(self.txn, genesis).is_some() {
  return; // Already completed, skip processing
}
```

**Testing Recommendations:**
1. Add integration tests that crash the coordinator between DkgConfirmed processing and transaction commit
2. Verify cached preprocesses are deleted or not reused on restart
3. Test that multiple calls to `complete()` for the same attempt either fail safe or use fresh nonces

### Proof of Concept

**Exploitation Steps:**

1. **Setup:** Validator set is performing DKG confirmation signing for attempt N
   - Cached preprocess exists in DB with key `(b"DkgConfirmer", N)`
   - All validators have published DkgShares with confirmation nonces

2. **Trigger Error Path:**
   - Byzantine validator publishes DkgConfirmed with invalid confirmation_share
   - Honest validator's coordinator processes all DkgConfirmed transactions
   - `handle_application_tx()` accumulates shares and calls `confirmer.complete()`
   - Inside `complete()`: calls `share_internal()` → `preprocess_internal()` → retrieves cached preprocess from DB → calls `machine.sign()` with nonces derived from seed
   - **First nonce use occurs**
   - `complete()` returns `Err(p)` for invalid participant
   - RemoveParticipantDueToDkg transaction published
   - Function returns early

3. **Crash Before Commit:**
   - Coordinator crashes (OOM, power loss, SIGKILL, etc.)
   - Transaction not committed
   - `LastHandledBlock` not updated
   - `DkgLocallyCompleted` not set
   - **Cached preprocess still in database**

4. **Restart and Reprocess:**
   - Coordinator restarts
   - Scanner processes blocks from `LastHandledBlock`
   - Same block processed again
   - `handle_application_tx()` called again for DkgConfirmed transactions
   - Shares accumulated again (still in DataDb)
   - `confirmer.complete()` called again
   - `share_internal()` → `preprocess_internal()` → **retrieves SAME cached preprocess** → calls `machine.sign()` with SAME nonces
   - **Second nonce use occurs with identical nonces**

5. **Key Recovery:**
   - Attacker observes both signing attempts (shares s₁, s₂ with same nonce r)
   - For Schnorr signatures: s = r + c·x (where x is private key share)
   - With same r but different challenges c₁, c₂ (due to different messages/contexts):
     - s₁ = r + c₁·x
     - s₂ = r + c₂·x
   - Solve for x: x = (s₁ - s₂) / (c₁ - c₂)
   - **Private key share recovered**

**Expected vs Actual Behavior:**

Expected: Each signing attempt uses fresh nonces, preventing key recovery even if multiple attempts are observed.

Actual: The same cached preprocess seed generates identical nonces across multiple signing attempts for the same attempt number, enabling trivial private key share recovery.

### Citations

**File:** coordinator/src/tributary/signing_protocol.rs (L25-54)
```rust
  As for safety, it is explicitly unsafe to reuse nonces across signing sessions. This raises
  concerns regarding our re-execution which is dependent on fixed nonces. Safety is derived from
  the nonces being context-bound under a BFT protocol. The flow is as follows:

  1) Decide the nonce.
  2) Publish the nonces' commitments, receiving everyone elses *and potentially the message to be
     signed*.
  3) Sign and publish the signature share.

  In order for nonce re-use to occur, the received nonce commitments (or the message to be signed)
  would have to be distinct and sign would have to be called again.

  Before we act on any received messages, they're ordered and finalized by a BFT algorithm. The
  only way to operate on distinct received messages would be if:

  1) A logical flaw exists, letting new messages over write prior messages
  2) A reorganization occurred from chain A to chain B, and with it, different messages

  Reorganizations are not supported, as BFT is assumed by the presence of a BFT algorithm. While
  a significant amount of processes may be byzantine, leading to BFT being broken, that still will
  not trigger a reorganization. The only way to move to a distinct chain, with distinct messages,
  would be by rebuilding the local process (this time following chain B). Upon any complete
  rebuild, we'd re-decide nonces, achieving safety. This does set a bound preventing partial
  rebuilds which is accepted.

  Additionally, to ensure a rebuilt service isn't flagged as malicious, we have to check the
  commitments generated from the decided nonces are in fact its commitments on-chain (TODO).

  TODO: We also need to review how we're handling Processor preprocesses and likely implement the
  same on-chain-preprocess-matches-presumed-preprocess check before publishing shares.
```

**File:** coordinator/src/tributary/signing_protocol.rs (L100-148)
```rust
  fn preprocess_internal(
    &mut self,
    participants: &[<Ristretto as Ciphersuite>::G],
  ) -> (AlgorithmSignMachine<Ristretto, Schnorrkel>, [u8; 64]) {
    // Encrypt the cached preprocess as recovery of it will enable recovering the private key
    // While the DB isn't expected to be arbitrarily readable, it isn't a proper secret store and
    // shouldn't be trusted as one
    let mut encryption_key = {
      let mut encryption_key_preimage =
        Zeroizing::new(b"Cached Preprocess Encryption Key".to_vec());
      encryption_key_preimage.extend(self.context.encode());
      let repr = Zeroizing::new(self.key.to_repr());
      encryption_key_preimage.extend(repr.deref());
      Blake2s256::digest(&encryption_key_preimage)
    };
    let encryption_key_slice: &mut [u8] = encryption_key.as_mut();

    let algorithm = Schnorrkel::new(b"substrate");
    let keys: ThresholdKeys<Ristretto> =
      musig(musig_context(self.spec.set().into()), self.key.clone(), participants)
        .expect("signing for a set we aren't in/validator present multiple times")
        .into();

    if CachedPreprocesses::get(self.txn, &self.context).is_none() {
      let (machine, _) =
        AlgorithmMachine::new(algorithm.clone(), keys.clone()).preprocess(&mut OsRng);

      let mut cache = machine.cache();
      assert_eq!(cache.0.len(), 32);
      #[allow(clippy::needless_range_loop)]
      for b in 0 .. 32 {
        cache.0[b] ^= encryption_key_slice[b];
      }

      CachedPreprocesses::set(self.txn, &self.context, &cache.0);
    }

    let cached = CachedPreprocesses::get(self.txn, &self.context).unwrap();
    let mut cached: Zeroizing<[u8; 32]> = Zeroizing::new(cached);
    #[allow(clippy::needless_range_loop)]
    for b in 0 .. 32 {
      cached[b] ^= encryption_key_slice[b];
    }
    encryption_key_slice.zeroize();
    let (machine, preprocess) =
      AlgorithmSignMachine::from_cache(algorithm, keys, CachedPreprocess(cached));

    (machine, preprocess.serialize().try_into().unwrap())
  }
```

**File:** coordinator/src/tributary/signing_protocol.rs (L150-181)
```rust
  fn share_internal(
    &mut self,
    participants: &[<Ristretto as Ciphersuite>::G],
    mut serialized_preprocesses: HashMap<Participant, Vec<u8>>,
    msg: &[u8],
  ) -> Result<(AlgorithmSignatureMachine<Ristretto, Schnorrkel>, [u8; 32]), Participant> {
    let machine = self.preprocess_internal(participants).0;

    let mut participants = serialized_preprocesses.keys().copied().collect::<Vec<_>>();
    participants.sort();
    let mut preprocesses = HashMap::new();
    for participant in participants {
      preprocesses.insert(
        participant,
        machine
          .read_preprocess(&mut serialized_preprocesses.remove(&participant).unwrap().as_slice())
          .map_err(|_| participant)?,
      );
    }

    let (machine, share) = machine.sign(preprocesses, msg).map_err(|e| match e {
      FrostError::InternalError(e) => unreachable!("FrostError::InternalError {e}"),
      FrostError::InvalidParticipant(_, _) |
      FrostError::InvalidSigningSet(_) |
      FrostError::InvalidParticipantQuantity(_, _) |
      FrostError::DuplicatedParticipant(_) |
      FrostError::MissingParticipant(_) => unreachable!("{e:?}"),
      FrostError::InvalidPreprocess(p) | FrostError::InvalidShare(p) => p,
    })?;

    Ok((machine, share.serialize().try_into().unwrap()))
  }
```

**File:** coordinator/src/tributary/signing_protocol.rs (L312-327)
```rust
  pub(crate) fn complete(
    &mut self,
    preprocesses: HashMap<Participant, Vec<u8>>,
    key_pair: &KeyPair,
    shares: HashMap<Participant, Vec<u8>>,
  ) -> Result<[u8; 64], Participant> {
    let shares =
      threshold_i_map_to_keys_and_musig_i_map(self.spec, &self.removed, self.key, shares).1;

    let machine = self
      .share_internal(preprocesses, key_pair)
      .expect("trying to complete a machine which failed to preprocess")
      .0;

    DkgConfirmerSigningProtocol::<'_, T>::complete_internal(machine, shares)
  }
```

**File:** coordinator/src/tributary/handle.rs (L508-566)
```rust
      Transaction::DkgConfirmed { attempt, confirmation_share, signed } => {
        let Some(removed) = removed_as_of_dkg_attempt(self.txn, genesis, attempt) else {
          self.fatal_slash(signed.signer.to_bytes(), "DkgConfirmed with an unrecognized attempt");
          return;
        };

        let data_spec =
          DataSpecification { topic: Topic::DkgConfirmation, label: Label::Share, attempt };
        match self.handle_data(&removed, &data_spec, &confirmation_share.to_vec(), &signed) {
          Accumulation::Ready(DataSet::Participating(shares)) => {
            log::info!("got all DkgConfirmed for {}", hex::encode(genesis));

            let Some(removed) = removed_as_of_dkg_attempt(self.txn, genesis, attempt) else {
              panic!(
                "DkgConfirmed for everyone yet didn't have the removed parties for this attempt",
              );
            };

            let preprocesses = ConfirmationNonces::get(self.txn, genesis, attempt).unwrap();
            // TODO: This can technically happen under very very very specific timing as the txn
            // put happens before DkgConfirmed, yet the txn commit isn't guaranteed to
            let key_pair = DkgKeyPair::get(self.txn, genesis, attempt).expect(
              "in DkgConfirmed handling, which happens after everyone \
              (including us) fires DkgConfirmed, yet no confirming key pair",
            );
            let mut confirmer = DkgConfirmer::new(self.our_key, self.spec, self.txn, attempt)
              .expect("confirming DKG for unrecognized attempt");
            let sig = match confirmer.complete(preprocesses, &key_pair, shares) {
              Ok(sig) => sig,
              Err(p) => {
                let mut tx = Transaction::RemoveParticipantDueToDkg {
                  participant: self.spec.reverse_lookup_i(&removed, p).unwrap(),
                  signed: Transaction::empty_signed(),
                };
                tx.sign(&mut OsRng, genesis, self.our_key);
                self.publish_tributary_tx.publish_tributary_tx(tx).await;
                return;
              }
            };

            DkgLocallyCompleted::set(self.txn, genesis, &());

            self
              .publish_serai_tx
              .publish_set_keys(
                self.db,
                self.spec.set(),
                removed.into_iter().map(|key| key.to_bytes().into()).collect(),
                key_pair,
                sig.into(),
              )
              .await;
          }
          Accumulation::Ready(DataSet::NotParticipating) => {
            panic!("wasn't a participant in DKG confirmination shares")
          }
          Accumulation::NotReady => {}
        }
      }
```

**File:** coordinator/src/tributary/scanner.rs (L660-720)
```rust
#[allow(clippy::too_many_arguments)]
pub(crate) async fn handle_new_blocks<
  D: Db,
  Pro: Processors,
  PST: PublishSeraiTransaction,
  PTT: PTTTrait,
  RID: RIDTrait,
  P: P2p,
>(
  db: &mut D,
  key: &Zeroizing<<Ristretto as Ciphersuite>::F>,
  recognized_id: &RID,
  processors: &Pro,
  publish_serai_tx: &PST,
  publish_tributary_tx: &PTT,
  spec: &TributarySpec,
  tributary: &TributaryReader<D, Transaction>,
) {
  let genesis = tributary.genesis();
  let mut last_block = LastHandledBlock::get(db, genesis).unwrap_or(genesis);
  let mut block_number = TributaryBlockNumber::get(db, last_block).unwrap_or(0);
  while let Some(next) = tributary.block_after(&last_block) {
    let block = tributary.block(&next).unwrap();
    block_number += 1;

    // Make sure we have all of the provided transactions for this block
    for tx in &block.transactions {
      // Provided TXs will appear first in the Block, so we can break after we hit a non-Provided
      let TransactionKind::Provided(order) = tx.kind() else {
        break;
      };

      // make sure we have all the provided txs in this block locally
      if !tributary.locally_provided_txs_in_block(&block.hash(), order) {
        return;
      }
    }

    let mut db_clone = db.clone();
    let mut txn = db_clone.txn();
    TributaryBlockNumber::set(&mut txn, next, &block_number);
    (TributaryBlockHandler {
      db,
      txn: &mut txn,
      spec,
      our_key: key,
      recognized_id,
      processors,
      publish_serai_tx,
      publish_tributary_tx,
      block,
      block_number,
      _p2p: PhantomData::<P>,
    })
    .handle()
    .await;
    last_block = next;
    LastHandledBlock::set(&mut txn, genesis, &next);
    txn.commit();
  }
}
```

**File:** crypto/frost/src/sign.rs (L83-87)
```rust
/// A cached preprocess.
///
/// A preprocess MUST only be used once. Reuse will enable third-party recovery of your private
/// key share. Additionally, this MUST be handled with the same security as your private key share,
/// as knowledge of it also enables recovery.
```

**File:** spec/cryptography/FROST.md (L51-55)
```markdown
Reusing preprocesses would enable a third-party to recover your private key
share. Accordingly, you MUST not reuse preprocesses. Third-party knowledge of
your preprocess would also enable their recovery of your private key share.
Accordingly, you MUST treat cached preprocesses with the same security as your
private key share.
```
